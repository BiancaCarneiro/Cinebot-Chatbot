{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports iniciais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\bianc\\desktop\\chatbot-nlp\\.venv\\lib\\site-packages (2.2.1)\n",
      "Requirement already satisfied: nltk in c:\\users\\bianc\\desktop\\chatbot-nlp\\.venv\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: torch in c:\\users\\bianc\\desktop\\chatbot-nlp\\.venv\\lib\\site-packages (2.2.1)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\bianc\\desktop\\chatbot-nlp\\.venv\\lib\\site-packages (3.8.3)\n",
      "Requirement already satisfied: seaborn in c:\\users\\bianc\\desktop\\chatbot-nlp\\.venv\\lib\\site-packages (0.13.2)\n",
      "Requirement already satisfied: wordcloud in c:\\users\\bianc\\desktop\\chatbot-nlp\\.venv\\lib\\site-packages (1.9.3)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\bianc\\desktop\\chatbot-nlp\\.venv\\lib\\site-packages (1.4.1.post1)\n",
      "Requirement already satisfied: evaluate in c:\\users\\bianc\\desktop\\chatbot-nlp\\.venv\\lib\\site-packages (0.4.1)\n",
      "Requirement already satisfied: datasets in c:\\users\\bianc\\desktop\\chatbot-nlp\\.venv\\lib\\site-packages (2.17.1)\n",
      "Requirement already satisfied: transformers[torch] in c:\\users\\bianc\\desktop\\chatbot-nlp\\.venv\\lib\\site-packages (4.38.1)\n",
      "Requirement already satisfied: numpy<2,>=1.22.4 in c:\\users\\bianc\\desktop\\chatbot-nlp\\.venv\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\bianc\\desktop\\chatbot-nlp\\.venv\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\bianc\\desktop\\chatbot-nlp\\.venv\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\bianc\\desktop\\chatbot-nlp\\.venv\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: click in c:\\users\\bianc\\desktop\\chatbot-nlp\\.venv\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\bianc\\desktop\\chatbot-nlp\\.venv\\lib\\site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\bianc\\desktop\\chatbot-nlp\\.venv\\lib\\site-packages (from nltk) (2023.12.25)\n",
      "Requirement already satisfied: tqdm in c:\\users\\bianc\\desktop\\chatbot-nlp\\.venv\\lib\\site-packages (from nltk) (4.66.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\bianc\\desktop\\chatbot-nlp\\.venv\\lib\\site-packages (from transformers[torch]) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in c:\\users\\bianc\\desktop\\chatbot-nlp\\.venv\\lib\\site-packages (from transformers[torch]) (0.21.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\bianc\\desktop\\chatbot-nlp\\.venv\\lib\\site-packages (from transformers[torch]) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\bianc\\desktop\\chatbot-nlp\\.venv\\lib\\site-packages (from transformers[torch]) (6.0.1)\n",
      "Requirement already satisfied: requests in c:\\users\\bianc\\desktop\\chatbot-nlp\\.venv\\lib\\site-packages (from transformers[torch]) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in c:\\users\\bianc\\desktop\\chatbot-nlp\\.venv\\lib\\site-packages (from transformers[torch]) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\bianc\\desktop\\chatbot-nlp\\.venv\\lib\\site-packages (from transformers[torch]) (0.4.2)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in c:\\users\\bianc\\desktop\\chatbot-nlp\\.venv\\lib\\site-packages (from transformers[torch]) (0.27.2)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\bianc\\desktop\\chatbot-nlp\\.venv\\lib\\site-packages (from torch) (4.10.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\bianc\\desktop\\chatbot-nlp\\.venv\\lib\\site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\bianc\\desktop\\chatbot-nlp\\.venv\\lib\\site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\bianc\\desktop\\chatbot-nlp\\.venv\\lib\\site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\bianc\\desktop\\chatbot-nlp\\.venv\\lib\\site-packages (from torch) (2023.10.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\bianc\\desktop\\chatbot-nlp\\.venv\\lib\\site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\bianc\\desktop\\chatbot-nlp\\.venv\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\bianc\\desktop\\chatbot-nlp\\.venv\\lib\\site-packages (from matplotlib) (4.49.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\bianc\\desktop\\chatbot-nlp\\.venv\\lib\\site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\bianc\\desktop\\chatbot-nlp\\.venv\\lib\\site-packages (from matplotlib) (10.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\bianc\\desktop\\chatbot-nlp\\.venv\\lib\\site-packages (from matplotlib) (3.1.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\bianc\\desktop\\chatbot-nlp\\.venv\\lib\\site-packages (from scikit-learn) (1.12.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\bianc\\desktop\\chatbot-nlp\\.venv\\lib\\site-packages (from scikit-learn) (3.3.0)\n",
      "Requirement already satisfied: dill in c:\\users\\bianc\\desktop\\chatbot-nlp\\.venv\\lib\\site-packages (from evaluate) (0.3.8)\n",
      "Requirement already satisfied: xxhash in c:\\users\\bianc\\desktop\\chatbot-nlp\\.venv\\lib\\site-packages (from evaluate) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\bianc\\desktop\\chatbot-nlp\\.venv\\lib\\site-packages (from evaluate) (0.70.16)\n",
      "Requirement already satisfied: responses<0.19 in c:\\users\\bianc\\desktop\\chatbot-nlp\\.venv\\lib\\site-packages (from evaluate) (0.18.0)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in c:\\users\\bianc\\desktop\\chatbot-nlp\\.venv\\lib\\site-packages (from datasets) (15.0.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in c:\\users\\bianc\\desktop\\chatbot-nlp\\.venv\\lib\\site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\bianc\\desktop\\chatbot-nlp\\.venv\\lib\\site-packages (from datasets) (3.9.3)\n",
      "Requirement already satisfied: psutil in c:\\users\\bianc\\desktop\\chatbot-nlp\\.venv\\lib\\site-packages (from accelerate>=0.21.0->transformers[torch]) (5.9.8)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\bianc\\desktop\\chatbot-nlp\\.venv\\lib\\site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\bianc\\desktop\\chatbot-nlp\\.venv\\lib\\site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\bianc\\desktop\\chatbot-nlp\\.venv\\lib\\site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\bianc\\desktop\\chatbot-nlp\\.venv\\lib\\site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\bianc\\desktop\\chatbot-nlp\\.venv\\lib\\site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in c:\\users\\bianc\\desktop\\chatbot-nlp\\.venv\\lib\\site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\bianc\\desktop\\chatbot-nlp\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\bianc\\desktop\\chatbot-nlp\\.venv\\lib\\site-packages (from requests->transformers[torch]) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\bianc\\desktop\\chatbot-nlp\\.venv\\lib\\site-packages (from requests->transformers[torch]) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\bianc\\desktop\\chatbot-nlp\\.venv\\lib\\site-packages (from requests->transformers[torch]) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\bianc\\desktop\\chatbot-nlp\\.venv\\lib\\site-packages (from requests->transformers[torch]) (2024.2.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\bianc\\desktop\\chatbot-nlp\\.venv\\lib\\site-packages (from tqdm->nltk) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\bianc\\desktop\\chatbot-nlp\\.venv\\lib\\site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\bianc\\desktop\\chatbot-nlp\\.venv\\lib\\site-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install pandas nltk transformers[torch] torch matplotlib seaborn wordcloud scikit-learn evaluate datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\bianc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\bianc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import nltk # pip3 install nltk\n",
    "from LSTM import *\n",
    "from transformers_finetune import *\n",
    "from common import *\n",
    "\n",
    "random.seed(42)\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_DATASET_NER = \"../datasets/NER_dataset.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>annotations</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[De, jeito, nenhum.], [ENTITY_PREFERENCE, ENT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[Super-herói, de, ficção, científica], [ENTIT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[[Eu, gosto], [ENTITY_PREFERENCE, ENTITY_PREFE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[[Bem,, como, eu, disse, eu, eu, gosto, princi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[[Sim,, absolutamente.], [ENTITY_PREFERENCE, E...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         annotations\n",
       "0  [[De, jeito, nenhum.], [ENTITY_PREFERENCE, ENT...\n",
       "1  [[Super-herói, de, ficção, científica], [ENTIT...\n",
       "2  [[Eu, gosto], [ENTITY_PREFERENCE, ENTITY_PREFE...\n",
       "3  [[Bem,, como, eu, disse, eu, eu, gosto, princi...\n",
       "4  [[Sim,, absolutamente.], [ENTITY_PREFERENCE, E..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_json(PATH_DATASET_NER)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_df = df[\"annotations\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analisando o dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(104,\n",
       " [['De', 'jeito', 'nenhum.'],\n",
       "  ['Super-herói', 'de', 'ficção', 'científica'],\n",
       "  ['Eu', 'gosto'],\n",
       "  ['Bem,',\n",
       "   'como',\n",
       "   'eu',\n",
       "   'disse',\n",
       "   'eu',\n",
       "   'eu',\n",
       "   'gosto',\n",
       "   'principalmente',\n",
       "   'de',\n",
       "   'comédias',\n",
       "   'porque',\n",
       "   'tira',\n",
       "   'minha',\n",
       "   'mente',\n",
       "   'das',\n",
       "   'coisas',\n",
       "   'do',\n",
       "   'dia',\n",
       "   'a',\n",
       "   'dia',\n",
       "   'e',\n",
       "   'eu',\n",
       "   'gosto',\n",
       "   'dos',\n",
       "   'documentários'],\n",
       "  ['Sim,', 'absolutamente.'],\n",
       "  ['filmes', 'de', 'comédia'],\n",
       "  ['Eu',\n",
       "   'realmente',\n",
       "   'gosto',\n",
       "   'de',\n",
       "   'comédias',\n",
       "   'e',\n",
       "   'filmes',\n",
       "   'de',\n",
       "   'ficção',\n",
       "   'científica'],\n",
       "  ['claro', 'com', 'a', 'primeira', 'vez', 'que', 'Homem', 'de', 'Ferro'],\n",
       "  ['Filmes',\n",
       "   'como',\n",
       "   'filmes',\n",
       "   'de',\n",
       "   'Adam',\n",
       "   'Sandler',\n",
       "   'ou',\n",
       "   'filmes',\n",
       "   'de',\n",
       "   'Jim',\n",
       "   'Carrey'],\n",
       "  ['Eu',\n",
       "   'gosto',\n",
       "   'da',\n",
       "   'história',\n",
       "   'de',\n",
       "   'uma',\n",
       "   'família',\n",
       "   'de',\n",
       "   'super-heróis',\n",
       "   'tentando',\n",
       "   'levar',\n",
       "   'uma',\n",
       "   'vida',\n",
       "   'normal',\n",
       "   ',',\n",
       "   'e',\n",
       "   'eu',\n",
       "   'gosto',\n",
       "   'da',\n",
       "   'animação.',\n",
       "   'Amo',\n",
       "   'tudo',\n",
       "   'da',\n",
       "   'Pixar']])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_entries = []\n",
    "\n",
    "for element in df.annotations:\n",
    "    text_entries.append(element[0])\n",
    "\n",
    "len(text_entries), text_entries[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'de jeito n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conc_text = \"\\n\".join([\" \".join(entries) for entries in text_entries]).lower()\n",
    "conc_text[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['de',\n",
       " 'jeito',\n",
       " 'nenhum',\n",
       " 'superherói',\n",
       " 'de',\n",
       " 'ficção',\n",
       " 'científica',\n",
       " 'eu',\n",
       " 'gosto',\n",
       " 'bem']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_words = nltk.tokenize.word_tokenize(remove_punctuation(conc_text), language=\"portuguese\")\n",
    "tokenized_words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'à',\n",
       " 'ao',\n",
       " 'aos',\n",
       " 'aquela',\n",
       " 'aquelas',\n",
       " 'aquele',\n",
       " 'aqueles',\n",
       " 'aquilo',\n",
       " 'as']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords_pt = nltk.corpus.stopwords.words('portuguese')\n",
    "stopwords_pt[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tamanho do vocabulário"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O tamanho do vocabulário é 443\n"
     ]
    }
   ],
   "source": [
    "print(f\"O tamanho do vocabulário é {len(set(tokenized_words))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tamanho das Sentenças"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_hist([len(phrase) for phrase in text_entries], title=\"Histograma: Número de Caracteres\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'translate'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m plot_hist([\u001b[38;5;28mlen\u001b[39m(nltk\u001b[38;5;241m.\u001b[39mtokenize\u001b[38;5;241m.\u001b[39mword_tokenize(remove_punctuation(phrase), language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mportuguese\u001b[39m\u001b[38;5;124m\"\u001b[39m)) \u001b[38;5;28;01mfor\u001b[39;00m phrase \u001b[38;5;129;01min\u001b[39;00m text_entries], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHistograma: Número de Tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[12], line 1\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[1;32m----> 1\u001b[0m plot_hist([\u001b[38;5;28mlen\u001b[39m(nltk\u001b[38;5;241m.\u001b[39mtokenize\u001b[38;5;241m.\u001b[39mword_tokenize(\u001b[43mremove_punctuation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mphrase\u001b[49m\u001b[43m)\u001b[49m, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mportuguese\u001b[39m\u001b[38;5;124m\"\u001b[39m)) \u001b[38;5;28;01mfor\u001b[39;00m phrase \u001b[38;5;129;01min\u001b[39;00m text_entries], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHistograma: Número de Tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\bianc\\Desktop\\Chatbot-NLP\\modelos\\common.py:11\u001b[0m, in \u001b[0;36mremove_punctuation\u001b[1;34m(input_string)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mremove_punctuation\u001b[39m(input_string):\n\u001b[0;32m     10\u001b[0m     translator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m\u001b[38;5;241m.\u001b[39mmaketrans(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, string\u001b[38;5;241m.\u001b[39mpunctuation)\n\u001b[1;32m---> 11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minput_string\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranslate\u001b[49m(translator)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'translate'"
     ]
    }
   ],
   "source": [
    "plot_hist([len(nltk.tokenize.word_tokenize(remove_punctuation(phrase), language=\"portuguese\")) for phrase in text_entries], \"Histograma: Número de Tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plot_hist' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mplot_hist\u001b[49m([\u001b[38;5;28mlen\u001b[39m(tk) \u001b[38;5;28;01mfor\u001b[39;00m tk \u001b[38;5;129;01min\u001b[39;00m tokenized_words], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHistograma: Tamanho dos Tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plot_hist' is not defined"
     ]
    }
   ],
   "source": [
    "plot_hist([len(tk) for tk in tokenized_words], \"Histograma: Tamanho dos Tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Palavras mais comuns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plot_most_frequent_word' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mplot_most_frequent_word\u001b[49m(tokenized_words, stopwords_pt)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plot_most_frequent_word' is not defined"
     ]
    }
   ],
   "source": [
    "plot_most_frequent_word(tokenized_words, stopwords_pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cloud_of_words' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mcloud_of_words\u001b[49m(conc_text, stopwords_pt)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'cloud_of_words' is not defined"
     ]
    }
   ],
   "source": [
    "cloud_of_words(conc_text, stopwords_pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Separando o dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [[Eu, gosto, de, filmes, de, aventura], [ENTIT...\n",
       "1    [[É, só, isso., Eu, não, sou, muito, fã, de, f...\n",
       "2    [[Foi, Houve, muitos, super-heróis, e, ação, n...\n",
       "Name: annotations, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.shuffle(ner_df)\n",
    "ner_df[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(62, 21, 21)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = ner_df[:int(len(ner_df)*0.6)]\n",
    "df_test = ner_df[int(len(ner_df)*0.6):int(len(ner_df)*0.8)]\n",
    "df_val = ner_df[int(len(ner_df)*0.8):]\n",
    "\n",
    "len(df_train), len(df_test), len(df_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Treinando o Extrator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_ix, tag_to_ix = one_hot_encoding_mapper(ner_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 32\n",
    "HIDDEN_DIM = 16\n",
    "OUTPUT_DIM = 5  # Número de tags de saída\n",
    "NUM_EPOCHS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treinando LSTM no device cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2/100 [00:00<00:06, 15.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Validation Loss: 1.4391574121656872\n",
      "Epoch 2, Validation Loss: 1.3276747947647458\n",
      "Epoch 3, Validation Loss: 1.2615800812130882\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 4/100 [00:00<00:05, 17.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Validation Loss: 1.2151458462079365\n",
      "Epoch 5, Validation Loss: 1.1839693131900968\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 6/100 [00:00<00:05, 18.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Validation Loss: 1.1690844779922849\n",
      "Epoch 7, Validation Loss: 1.1690947441827684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 8/100 [00:00<00:04, 18.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Validation Loss: 1.1802806173052107\n",
      "Epoch 9, Validation Loss: 1.2008491470700218\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 10/100 [00:00<00:05, 16.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Validation Loss: 1.232724966037841\n",
      "Epoch 11, Validation Loss: 1.273422964272045\n",
      "Early stopping!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = train_lstm(NUM_EPOCHS, df_train, df_val, word_to_ix, tag_to_ix, EMBEDDING_DIM, HIDDEN_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.58984375\n"
     ]
    }
   ],
   "source": [
    "test_LSTM(model, word_to_ix, tag_to_ix, df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['ENTITY_PREFERENCE', 'O', 'ENTITY_DESCRIPTION', 'ENTITY_OTHER', 'ENTITY_NAME', 'ENTITY_ACTOR'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_to_ix.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_pd = pd.DataFrame.from_dict({\"tokens\":[entry[0] for entry in df_train], \"label\":[[tag_to_ix[lb] for lb in entry[1]] for entry in df_train]})\n",
    "df_test_pd = pd.DataFrame.from_dict({\"tokens\":[entry[0] for entry in df_test], \"label\":[[tag_to_ix[lb] for lb in entry[1]] for entry in df_test]})\n",
    "df_val_pd = pd.DataFrame.from_dict({\"tokens\":[entry[0] for entry in df_val], \"label\":[[tag_to_ix[lb] for lb in entry[1]] for entry in df_val]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Eu, realmente, gosto, de, comédias, e, filmes...</td>\n",
       "      <td>[0, 0, 0, 1, 2, 1, 2, 2, 2, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Deixe-me, pensar, por, um, momento., Tem, ess...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 4, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[Eu, sou, fã, de, filmes, de, aventura]</td>\n",
       "      <td>[0, 0, 0, 2, 2, 2, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[Eu, sou, fã, de, filmes, de, ação, cheios, de...</td>\n",
       "      <td>[0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[Sexta-feira, 13, é, um, filme, de, terror., É...</td>\n",
       "      <td>[4, 4, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              tokens  \\\n",
       "0  [Eu, realmente, gosto, de, comédias, e, filmes...   \n",
       "1  [Deixe-me, pensar, por, um, momento., Tem, ess...   \n",
       "2            [Eu, sou, fã, de, filmes, de, aventura]   \n",
       "3  [Eu, sou, fã, de, filmes, de, ação, cheios, de...   \n",
       "4  [Sexta-feira, 13, é, um, filme, de, terror., É...   \n",
       "\n",
       "                                               label  \n",
       "0                     [0, 0, 0, 1, 2, 1, 2, 2, 2, 2]  \n",
       "1                  [1, 1, 1, 1, 1, 1, 1, 1, 1, 4, 4]  \n",
       "2                              [0, 0, 0, 2, 2, 2, 2]  \n",
       "3                  [0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2]  \n",
       "4  [4, 4, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_pd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = TrainTransformers(df_train_pd, df_test_pd, df_val_pd, list(tag_to_ix.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "We couldn't connect to 'https://huggingface.co' to load this file, couldn't find it in the cached files and it looks like Luciano/bertimbau-base-lener_br is not the path to a directory containing a file named config.json.\nCheckout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\bianc\\Desktop\\Chatbot-NLP\\.venv\\lib\\site-packages\\huggingface_hub\\utils\\_errors.py:304\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[1;34m(response, endpoint_name)\u001b[0m\n\u001b[0;32m    303\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 304\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\bianc\\Desktop\\Chatbot-NLP\\.venv\\lib\\site-packages\\requests\\models.py:1021\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1020\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[1;32m-> 1021\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[1;31mHTTPError\u001b[0m: 503 Server Error: Service Temporarily Unavailable for url: https://huggingface.co/Luciano/bertimbau-base-lener_br/resolve/main/config.json",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mHfHubHTTPError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\bianc\\Desktop\\Chatbot-NLP\\.venv\\lib\\site-packages\\huggingface_hub\\file_download.py:1261\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[1;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout, endpoint)\u001b[0m\n\u001b[0;32m   1260\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1261\u001b[0m     metadata \u001b[38;5;241m=\u001b[39m \u001b[43mget_hf_file_metadata\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1262\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1263\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1264\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1265\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1266\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlibrary_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlibrary_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1267\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlibrary_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlibrary_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1268\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1269\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1270\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m http_error:\n\u001b[0;32m   1271\u001b[0m     \u001b[38;5;66;03m# Cache the non-existence of the file and raise\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\bianc\\Desktop\\Chatbot-NLP\\.venv\\lib\\site-packages\\huggingface_hub\\utils\\_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    116\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m--> 118\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\bianc\\Desktop\\Chatbot-NLP\\.venv\\lib\\site-packages\\huggingface_hub\\file_download.py:1667\u001b[0m, in \u001b[0;36mget_hf_file_metadata\u001b[1;34m(url, token, proxies, timeout, library_name, library_version, user_agent)\u001b[0m\n\u001b[0;32m   1666\u001b[0m \u001b[38;5;66;03m# Retrieve metadata\u001b[39;00m\n\u001b[1;32m-> 1667\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1668\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHEAD\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1669\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1670\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1671\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1672\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1673\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1674\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1675\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1676\u001b[0m hf_raise_for_status(r)\n",
      "File \u001b[1;32mc:\\Users\\bianc\\Desktop\\Chatbot-NLP\\.venv\\lib\\site-packages\\huggingface_hub\\file_download.py:385\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[1;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[0;32m    384\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m follow_relative_redirects:\n\u001b[1;32m--> 385\u001b[0m     response \u001b[38;5;241m=\u001b[39m _request_wrapper(\n\u001b[0;32m    386\u001b[0m         method\u001b[38;5;241m=\u001b[39mmethod,\n\u001b[0;32m    387\u001b[0m         url\u001b[38;5;241m=\u001b[39murl,\n\u001b[0;32m    388\u001b[0m         follow_relative_redirects\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    389\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams,\n\u001b[0;32m    390\u001b[0m     )\n\u001b[0;32m    392\u001b[0m     \u001b[38;5;66;03m# If redirection, we redirect only relative paths.\u001b[39;00m\n\u001b[0;32m    393\u001b[0m     \u001b[38;5;66;03m# This is useful in case of a renamed repository.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\bianc\\Desktop\\Chatbot-NLP\\.venv\\lib\\site-packages\\huggingface_hub\\file_download.py:409\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[1;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[0;32m    408\u001b[0m response \u001b[38;5;241m=\u001b[39m get_session()\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[1;32m--> 409\u001b[0m \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    410\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32mc:\\Users\\bianc\\Desktop\\Chatbot-NLP\\.venv\\lib\\site-packages\\huggingface_hub\\utils\\_errors.py:362\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[1;34m(response, endpoint_name)\u001b[0m\n\u001b[0;32m    360\u001b[0m \u001b[38;5;66;03m# Convert `HTTPError` into a `HfHubHTTPError` to display request information\u001b[39;00m\n\u001b[0;32m    361\u001b[0m \u001b[38;5;66;03m# as well (request id and/or server error message)\u001b[39;00m\n\u001b[1;32m--> 362\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m HfHubHTTPError(\u001b[38;5;28mstr\u001b[39m(e), response\u001b[38;5;241m=\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[1;31mHfHubHTTPError\u001b[0m: 503 Server Error: Service Temporarily Unavailable for url: https://huggingface.co/Luciano/bertimbau-base-lener_br/resolve/main/config.json",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mLocalEntryNotFoundError\u001b[0m                   Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\bianc\\Desktop\\Chatbot-NLP\\.venv\\lib\\site-packages\\transformers\\utils\\hub.py:398\u001b[0m, in \u001b[0;36mcached_file\u001b[1;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[0;32m    396\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    397\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[1;32m--> 398\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    399\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    400\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    401\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    402\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    403\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    404\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    405\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    406\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    407\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    411\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    412\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\bianc\\Desktop\\Chatbot-NLP\\.venv\\lib\\site-packages\\huggingface_hub\\utils\\_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    116\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m--> 118\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\bianc\\Desktop\\Chatbot-NLP\\.venv\\lib\\site-packages\\huggingface_hub\\file_download.py:1406\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[1;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout, endpoint)\u001b[0m\n\u001b[0;32m   1404\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1405\u001b[0m         \u001b[38;5;66;03m# Otherwise: most likely a connection issue or Hub downtime => let's warn the user\u001b[39;00m\n\u001b[1;32m-> 1406\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LocalEntryNotFoundError(\n\u001b[0;32m   1407\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error happened while trying to locate the file on the Hub and we cannot find the requested files\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1408\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m in the local cache. Please check your connection and try again or make sure your Internet connection\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1409\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is on.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1410\u001b[0m         ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhead_call_error\u001b[39;00m\n\u001b[0;32m   1412\u001b[0m \u001b[38;5;66;03m# From now on, etag and commit_hash are not None.\u001b[39;00m\n",
      "\u001b[1;31mLocalEntryNotFoundError\u001b[0m: An error happened while trying to locate the file on the Hub and we cannot find the requested files in the local cache. Please check your connection and try again or make sure your Internet connection is on.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 11\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# https://huggingface.co/learn/nlp-course/chapter7/2?fw=pt\u001b[39;00m\n\u001b[0;32m      9\u001b[0m MODEL \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLuciano/bertimbau-base-lener_br\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mTrainTransformers\u001b[39;00m():\n\u001b[0;32m     12\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(MODEL)\n\u001b[0;32m     13\u001b[0m     model \u001b[38;5;241m=\u001b[39m AutoModelForTokenClassification\u001b[38;5;241m.\u001b[39mfrom_pretrained(MODEL)\n",
      "Cell \u001b[1;32mIn[21], line 12\u001b[0m, in \u001b[0;36mTrainTransformers\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mTrainTransformers\u001b[39;00m():\n\u001b[1;32m---> 12\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mAutoTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMODEL\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m     model \u001b[38;5;241m=\u001b[39m AutoModelForTokenClassification\u001b[38;5;241m.\u001b[39mfrom_pretrained(MODEL)\n\u001b[0;32m     14\u001b[0m     data_collator \u001b[38;5;241m=\u001b[39m DataCollatorForTokenClassification(tokenizer)\n",
      "File \u001b[1;32mc:\\Users\\bianc\\Desktop\\Chatbot-NLP\\.venv\\lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:782\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m    780\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config_tokenizer_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    781\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, PretrainedConfig):\n\u001b[1;32m--> 782\u001b[0m         config \u001b[38;5;241m=\u001b[39m AutoConfig\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m    783\u001b[0m             pretrained_model_name_or_path, trust_remote_code\u001b[38;5;241m=\u001b[39mtrust_remote_code, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    784\u001b[0m         )\n\u001b[0;32m    785\u001b[0m     config_tokenizer_class \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mtokenizer_class\n\u001b[0;32m    786\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(config, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAutoTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config\u001b[38;5;241m.\u001b[39mauto_map:\n",
      "File \u001b[1;32mc:\\Users\\bianc\\Desktop\\Chatbot-NLP\\.venv\\lib\\site-packages\\transformers\\models\\auto\\configuration_auto.py:1111\u001b[0m, in \u001b[0;36mAutoConfig.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m   1108\u001b[0m trust_remote_code \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrust_remote_code\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m   1109\u001b[0m code_revision \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcode_revision\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m-> 1111\u001b[0m config_dict, unused_kwargs \u001b[38;5;241m=\u001b[39m PretrainedConfig\u001b[38;5;241m.\u001b[39mget_config_dict(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1112\u001b[0m has_remote_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAutoConfig\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1113\u001b[0m has_local_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict \u001b[38;5;129;01mand\u001b[39;00m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m CONFIG_MAPPING\n",
      "File \u001b[1;32mc:\\Users\\bianc\\Desktop\\Chatbot-NLP\\.venv\\lib\\site-packages\\transformers\\configuration_utils.py:633\u001b[0m, in \u001b[0;36mPretrainedConfig.get_config_dict\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    631\u001b[0m original_kwargs \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(kwargs)\n\u001b[0;32m    632\u001b[0m \u001b[38;5;66;03m# Get config dict associated with the base config file\u001b[39;00m\n\u001b[1;32m--> 633\u001b[0m config_dict, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_get_config_dict(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    634\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict:\n\u001b[0;32m    635\u001b[0m     original_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\bianc\\Desktop\\Chatbot-NLP\\.venv\\lib\\site-packages\\transformers\\configuration_utils.py:688\u001b[0m, in \u001b[0;36mPretrainedConfig._get_config_dict\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    684\u001b[0m configuration_file \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_configuration_file\u001b[39m\u001b[38;5;124m\"\u001b[39m, CONFIG_NAME)\n\u001b[0;32m    686\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    687\u001b[0m     \u001b[38;5;66;03m# Load from local folder or from cache or download from model Hub and cache\u001b[39;00m\n\u001b[1;32m--> 688\u001b[0m     resolved_config_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    689\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    690\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfiguration_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    691\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    692\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    693\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    694\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    695\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    696\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    697\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    698\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    699\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    700\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    701\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    702\u001b[0m     commit_hash \u001b[38;5;241m=\u001b[39m extract_commit_hash(resolved_config_file, commit_hash)\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m:\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;66;03m# Raise any environment error raise by `cached_file`. It will have a helpful error message adapted to\u001b[39;00m\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;66;03m# the original exception.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\bianc\\Desktop\\Chatbot-NLP\\.venv\\lib\\site-packages\\transformers\\utils\\hub.py:441\u001b[0m, in \u001b[0;36mcached_file\u001b[1;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[0;32m    435\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    436\u001b[0m         resolved_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    437\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _raise_exceptions_for_missing_entries\n\u001b[0;32m    438\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _raise_exceptions_for_connection_errors\n\u001b[0;32m    439\u001b[0m     ):\n\u001b[0;32m    440\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m resolved_file\n\u001b[1;32m--> 441\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[0;32m    442\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWe couldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt connect to \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mHUGGINGFACE_CO_RESOLVE_ENDPOINT\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to load this file, couldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find it in the\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    443\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m cached files and it looks like \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not the path to a directory containing a file named\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    444\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfull_filename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mCheckout your internet connection or see how to run the library in offline mode at\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    445\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/docs/transformers/installation#offline-mode\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    446\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    447\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    448\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _raise_exceptions_for_missing_entries:\n",
      "\u001b[1;31mOSError\u001b[0m: We couldn't connect to 'https://huggingface.co' to load this file, couldn't find it in the cached files and it looks like Luciano/bertimbau-base-lener_br is not the path to a directory containing a file named config.json.\nCheckout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'."
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "from pandas import DataFrame\n",
    "from transformers import AutoModelForTokenClassification, AutoTokenizer, TrainingArguments, Trainer, DataCollatorForTokenClassification\n",
    "from datasets import Dataset, DatasetDict\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "# https://huggingface.co/learn/nlp-course/chapter7/2?fw=pt\n",
    "\n",
    "MODEL = \"microsoft/xtremedistil-l6-h256-uncased\"\n",
    "\n",
    "class TrainTransformers():\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "    model = AutoModelForTokenClassification.from_pretrained(MODEL)\n",
    "    data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "    \n",
    "    def __init__(self, train_df:DataFrame, test_df:DataFrame, val_df:DataFrame, label_list:list) -> None:\n",
    "        self.train_df = Dataset.from_pandas(train_df)\n",
    "        self.test_df = Dataset.from_pandas(test_df)\n",
    "        self.val_df = Dataset.from_pandas(val_df)\n",
    "        self.label_list = label_list\n",
    "    \n",
    "    def tokenize_and_align_labels(self, examples):\n",
    "        tokenized_inputs = self.tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True, max_length=128, padding=True)\n",
    "\n",
    "        labels = []\n",
    "        for i, label in enumerate(examples[\"label\"]):\n",
    "            word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "            previous_word_idx = None\n",
    "            label_ids = []\n",
    "            for word_idx in word_ids:\n",
    "                if word_idx is None:\n",
    "                    label_ids.append(-100)\n",
    "                elif word_idx != previous_word_idx:\n",
    "                    label_ids.append(label[word_idx])\n",
    "                else:\n",
    "                    label_ids.append(label[word_idx])\n",
    "                previous_word_idx = word_idx\n",
    "\n",
    "            labels.append(label_ids)\n",
    "\n",
    "        tokenized_inputs[\"labels\"] = labels\n",
    "        return tokenized_inputs\n",
    "\n",
    "\n",
    "    def train(self, model_name:str, lr:float=0.001, num_epochs:int=10, batch_size:int=500):\n",
    "        train_data = self.train_df.map(self.tokenize_and_align_labels, batched=True)\n",
    "        val_data = self.val_df.map(self.tokenize_and_align_labels, batched=True)\n",
    "        \n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=model_name,\n",
    "            do_eval=True,\n",
    "            lr_scheduler_type=\"linear\",\n",
    "            no_cuda=False,\n",
    "            learning_rate=lr,\n",
    "            per_device_train_batch_size=batch_size,\n",
    "            per_device_eval_batch_size=batch_size,\n",
    "            num_train_epochs=3,\n",
    "            evaluation_strategy=\"epoch\"\n",
    "        )\n",
    "    \n",
    "        \n",
    "        def compute_metrics(p):\n",
    "            predictions, labels = p\n",
    "            predictions = np.argmax(predictions, axis=2)\n",
    "            \n",
    "            # Remove ignored index (special tokens)\n",
    "            true_predictions = [\n",
    "                [self.label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "                for prediction, label in zip(predictions, labels)\n",
    "            ]\n",
    "            true_labels = [\n",
    "                [self.label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "                for prediction, label in zip(predictions, labels)\n",
    "            ]\n",
    "            results = {\n",
    "                'accuracy': accuracy_score(true_labels, true_predictions),\n",
    "                'f1': f1_score(true_labels, true_predictions),\n",
    "                'classification_report': classification_report(true_labels, true_predictions)\n",
    "            }\n",
    "            return results\n",
    "        \n",
    "        trainer = Trainer(\n",
    "            model=self.model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_data,\n",
    "            eval_dataset=val_data,\n",
    "            tokenizer=self.tokenizer, \n",
    "            data_collator=self.data_collator,\n",
    "            compute_metrics=compute_metrics,\n",
    "        )\n",
    "        \n",
    "        trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = TrainTransformers(df_train_pd, df_test_pd, df_val_pd, list(tag_to_ix.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 62/62 [00:00<00:00, 5630.79 examples/s]\n",
      "Map: 100%|██████████| 21/21 [00:00<00:00, 4196.10 examples/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "BertForTokenClassification.forward() got an unexpected keyword argument 'label'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mner_cinema_bert\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[16], line 92\u001b[0m, in \u001b[0;36mTrainTransformers.train\u001b[1;34m(self, model_name, lr, num_epochs, batch_size)\u001b[0m\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[0;32m     82\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[0;32m     83\u001b[0m     model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel,\n\u001b[0;32m     84\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     89\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics,\n\u001b[0;32m     90\u001b[0m )\n\u001b[1;32m---> 92\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\bianc\\Desktop\\Chatbot-NLP\\.venv\\lib\\site-packages\\transformers\\trainer.py:1624\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1622\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   1623\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1624\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1625\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1626\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1627\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1628\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1629\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\bianc\\Desktop\\Chatbot-NLP\\.venv\\lib\\site-packages\\transformers\\trainer.py:1961\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1958\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[0;32m   1960\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[1;32m-> 1961\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1963\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   1964\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   1965\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[0;32m   1966\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   1967\u001b[0m ):\n\u001b[0;32m   1968\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   1969\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32mc:\\Users\\bianc\\Desktop\\Chatbot-NLP\\.venv\\lib\\site-packages\\transformers\\trainer.py:2902\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(self, model, inputs)\u001b[0m\n\u001b[0;32m   2899\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m   2901\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[1;32m-> 2902\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2904\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mn_gpu \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   2905\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mmean()  \u001b[38;5;66;03m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\bianc\\Desktop\\Chatbot-NLP\\.venv\\lib\\site-packages\\transformers\\trainer.py:2925\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[1;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[0;32m   2923\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2924\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 2925\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\n\u001b[0;32m   2926\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[0;32m   2927\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[0;32m   2928\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\bianc\\Desktop\\Chatbot-NLP\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\bianc\\Desktop\\Chatbot-NLP\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: BertForTokenClassification.forward() got an unexpected keyword argument 'label'"
     ]
    }
   ],
   "source": [
    "trainer.train(\"ner_cinema_bert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
